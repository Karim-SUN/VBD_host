model_name: VBD

# Data Config
# train_data_path: /home/huang/Mount_Disk/shy/vbd_dataset/train/processed
# val_data_path: /home/huang/Mount_Disk/shy/vbd_dataset/val/processed
# anchor_path: /home/huang/Mount_Disk/shy/VBD/vbd/data/cluster_64_center_dict.pkl
# log_dir: ./train_log
train_data_path: /home/karim/VBD/data/train
val_data_path: /home/karim/VBD/data/val
anchor_path: vbd/data/cluster_64_center_dict.pkl
log_dir: ./train_log

# Wandb Config
use_wandb: False
username: karim-sun
project: vbd

# Checkpoint Config
init_from: null

# Model Config
agents_len: 16
future_len: 80
action_len: 2
encoder_layers: 6

# Encoder
history_dropout: 0.4

# Task Probabilities for get_random_mask
task_probabilities:
  prediction: 0.8
  goal_conditioned: 0.08
  agent_reactive: 0.08
  reconstruction: 0.04

# Predictor
#prediction_type: sample
prediction_type: sample
encoder_version: v3 # v1: GRU, v2: MLP, v3: Self-Attention
gumbel_tau: 0.1

# Diffusion Params
diffusion_steps: 1000
#schedule_type: cosine  $$$
schedule_type: log
schedule_s: 0
schedule_e: 1
schedule_tau: 1

action_mean: [0.0, 0.0]
# action_std: [1.0, 0.15]
action_std: [1.0, 1.0]
clamp_value: 5.0

normalize_anchors: True
anchor_incre_min: [-0.2, -2]
anchor_incre_max: [7, 2]

# Training Params
seed: 21
batch_size: 1
num_workers: 16
#lr: 0.0002
lr: 0.0002
weight_decay: 0.01
epochs: 12
lr_warmup_step: 1000
lr_step_freq: 1000
#lr_step_gamma: 0.98
lr_step_gamma: 0.96
precision: "bf16-mixed"

train_encoder: True
train_denoiser: True
train_predictor: True
